2019-07-13 08:12:29,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = nilesh
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/etc/hadoop:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-codec-1.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-lang-2.6.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/avro-1.7.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jersey-server-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-io-2.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/asm-3.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/activation-1.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-net-3.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/guava-11.0.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jsp-api-2.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jsch-0.1.54.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/xz-1.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-digester-1.8.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/junit-4.11.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/log4j-1.2.17.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-cli-1.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/servlet-api-2.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jettison-1.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jetty-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jersey-core-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/paranamer-2.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jersey-json-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/xmlenc-0.52.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/gson-2.2.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/hadoop-common-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/asm-3.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/asm-3.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/activation-1.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/javax.inject-1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/xz-1.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jettison-1.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/guice-3.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/fst-2.50.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_101
************************************************************/
2019-07-13 08:12:29,065 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-07-13 08:12:29,825 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-07-13 08:12:30,369 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-07-13 08:12:30,476 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-07-13 08:12:30,476 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-07-13 08:12:30,488 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-07-13 08:12:30,489 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2019-07-13 08:12:30,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-07-13 08:12:30,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-07-13 08:12:30,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-07-13 08:12:30,535 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-07-13 08:12:30,689 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-07-13 08:12:30,704 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-07-13 08:12:30,736 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-07-13 08:12:30,749 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-07-13 08:12:30,751 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-07-13 08:12:30,751 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-07-13 08:12:30,751 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-07-13 08:12:30,781 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 46579
2019-07-13 08:12:30,782 INFO org.mortbay.log: jetty-6.1.26
2019-07-13 08:12:31,097 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:46579
2019-07-13 08:12:31,492 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-07-13 08:12:31,516 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-07-13 08:12:31,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = nilesh
2019-07-13 08:12:31,793 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-07-13 08:12:31,970 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-07-13 08:12:31,990 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-07-13 08:12:32,108 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-07-13 08:12:32,135 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-07-13 08:12:32,150 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-07-13 08:12:32,184 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-07-13 08:12:32,214 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-07-13 08:12:32,217 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-07-13 08:12:33,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-07-13 08:12:33,123 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-07-13 08:12:33,133 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-nilesh/dfs/data/in_use.lock acquired by nodename 12422@ubuntu
2019-07-13 08:12:33,134 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-nilesh/dfs/data is not formatted for namespace 701960640. Formatting...
2019-07-13 08:12:33,135 INFO org.apache.hadoop.hdfs.server.common.Storage: Generated new storageID DS-bec322a5-17a0-4e98-96d0-7b40436ece86 for directory /tmp/hadoop-nilesh/dfs/data
2019-07-13 08:12:33,371 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-900433139-127.0.1.1-1563029088600
2019-07-13 08:12:33,372 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /tmp/hadoop-nilesh/dfs/data/current/BP-900433139-127.0.1.1-1563029088600
2019-07-13 08:12:33,372 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /tmp/hadoop-nilesh/dfs/data/current/BP-900433139-127.0.1.1-1563029088600 is not formatted for BP-900433139-127.0.1.1-1563029088600. Formatting ...
2019-07-13 08:12:33,374 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-900433139-127.0.1.1-1563029088600 directory /tmp/hadoop-nilesh/dfs/data/current/BP-900433139-127.0.1.1-1563029088600/current
2019-07-13 08:12:33,404 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=701960640;bpid=BP-900433139-127.0.1.1-1563029088600;lv=-57;nsInfo=lv=-63;cid=CID-8634a4fd-fa4b-48a5-ac36-7c1520d55aa4;nsid=701960640;c=1563029088600;bpid=BP-900433139-127.0.1.1-1563029088600;dnuuid=null
2019-07-13 08:12:33,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID c5c8865c-7214-4b21-9137-798598342998
2019-07-13 08:12:33,628 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-bec322a5-17a0-4e98-96d0-7b40436ece86
2019-07-13 08:12:33,628 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-nilesh/dfs/data/current, StorageType: DISK
2019-07-13 08:12:33,649 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-07-13 08:12:33,692 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-07-13 08:12:33,692 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-900433139-127.0.1.1-1563029088600
2019-07-13 08:12:33,703 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-900433139-127.0.1.1-1563029088600 on volume /tmp/hadoop-nilesh/dfs/data/current...
2019-07-13 08:12:33,753 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-900433139-127.0.1.1-1563029088600 on /tmp/hadoop-nilesh/dfs/data/current: 49ms
2019-07-13 08:12:33,753 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-900433139-127.0.1.1-1563029088600: 61ms
2019-07-13 08:12:33,767 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-900433139-127.0.1.1-1563029088600 on volume /tmp/hadoop-nilesh/dfs/data/current...
2019-07-13 08:12:33,767 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /tmp/hadoop-nilesh/dfs/data/current/BP-900433139-127.0.1.1-1563029088600/current/replicas doesn't exist 
2019-07-13 08:12:33,768 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-900433139-127.0.1.1-1563029088600 on volume /tmp/hadoop-nilesh/dfs/data/current: 0ms
2019-07-13 08:12:33,768 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 13ms
2019-07-13 08:12:33,769 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-900433139-127.0.1.1-1563029088600 on volume /tmp/hadoop-nilesh/dfs/data
2019-07-13 08:12:33,792 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/tmp/hadoop-nilesh/dfs/data, DS-bec322a5-17a0-4e98-96d0-7b40436ece86): finished scanning block pool BP-900433139-127.0.1.1-1563029088600
2019-07-13 08:12:33,803 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 7/13/19 9:56 AM with interval of 21600000ms
2019-07-13 08:12:33,825 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-900433139-127.0.1.1-1563029088600 (Datanode Uuid c5c8865c-7214-4b21-9137-798598342998) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-07-13 08:12:33,934 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/tmp/hadoop-nilesh/dfs/data, DS-bec322a5-17a0-4e98-96d0-7b40436ece86): no suitable block pools found to scan.  Waiting 1814399835 ms.
2019-07-13 08:12:34,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-900433139-127.0.1.1-1563029088600 (Datanode Uuid c5c8865c-7214-4b21-9137-798598342998) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-07-13 08:12:34,020 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-07-13 08:12:34,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x3cb15c4b98f4eaf1,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 8 msec to generate and 128 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-07-13 08:12:34,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-900433139-127.0.1.1-1563029088600
2019-07-13 09:20:52,157 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1869ms
No GCs detected
2019-07-13 09:20:58,560 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1386ms
No GCs detected
2019-07-13 11:16:34,021 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-900433139-127.0.1.1-1563029088600:blk_1073741825_1001 src: /127.0.0.1:42940 dest: /127.0.0.1:50010
2019-07-13 11:16:34,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:42940, dest: /127.0.0.1:50010, bytes: 707, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1163989541_1, offset: 0, srvID: c5c8865c-7214-4b21-9137-798598342998, blockid: BP-900433139-127.0.1.1-1563029088600:blk_1073741825_1001, duration: 21811328
2019-07-13 11:16:34,159 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-900433139-127.0.1.1-1563029088600:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
2019-07-13 11:24:59,742 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-900433139-127.0.1.1-1563029088600 Total blocks: 1, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2019-07-13 11:34:20,455 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-900433139-127.0.1.1-1563029088600:blk_1073741826_1002 src: /127.0.0.1:43036 dest: /127.0.0.1:50010
2019-07-13 11:34:20,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43036, dest: /127.0.0.1:50010, bytes: 5028, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_36016501_1, offset: 0, srvID: c5c8865c-7214-4b21-9137-798598342998, blockid: BP-900433139-127.0.1.1-1563029088600:blk_1073741826_1002, duration: 17157402
2019-07-13 11:34:20,477 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-900433139-127.0.1.1-1563029088600:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating
2019-07-13 11:34:20,614 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-900433139-127.0.1.1-1563029088600:blk_1073741827_1003 src: /127.0.0.1:43038 dest: /127.0.0.1:50010
2019-07-13 11:34:20,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43038, dest: /127.0.0.1:50010, bytes: 109, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_36016501_1, offset: 0, srvID: c5c8865c-7214-4b21-9137-798598342998, blockid: BP-900433139-127.0.1.1-1563029088600:blk_1073741827_1003, duration: 5997391
2019-07-13 11:34:20,623 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-900433139-127.0.1.1-1563029088600:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating
2019-07-13 11:34:20,659 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-900433139-127.0.1.1-1563029088600:blk_1073741828_1004 src: /127.0.0.1:43040 dest: /127.0.0.1:50010
2019-07-13 11:34:20,673 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43040, dest: /127.0.0.1:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_36016501_1, offset: 0, srvID: c5c8865c-7214-4b21-9137-798598342998, blockid: BP-900433139-127.0.1.1-1563029088600:blk_1073741828_1004, duration: 5072535
2019-07-13 11:34:20,682 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-900433139-127.0.1.1-1563029088600:blk_1073741828_1004, type=LAST_IN_PIPELINE terminating
2019-07-13 11:34:20,887 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-900433139-127.0.1.1-1563029088600:blk_1073741829_1005 src: /127.0.0.1:43042 dest: /127.0.0.1:50010
2019-07-13 11:34:20,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43042, dest: /127.0.0.1:50010, bytes: 134200, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_36016501_1, offset: 0, srvID: c5c8865c-7214-4b21-9137-798598342998, blockid: BP-900433139-127.0.1.1-1563029088600:blk_1073741829_1005, duration: 20009116
2019-07-13 11:34:20,911 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-900433139-127.0.1.1-1563029088600:blk_1073741829_1005, type=LAST_IN_PIPELINE terminating
2019-07-13 11:34:34,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-900433139-127.0.1.1-1563029088600:blk_1073741830_1006 src: /127.0.0.1:43060 dest: /127.0.0.1:50010
2019-07-13 11:34:34,242 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43060, dest: /127.0.0.1:50010, bytes: 156087, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_492516669_1, offset: 0, srvID: c5c8865c-7214-4b21-9137-798598342998, blockid: BP-900433139-127.0.1.1-1563029088600:blk_1073741830_1006, duration: 16976175
2019-07-13 11:34:34,252 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-900433139-127.0.1.1-1563029088600:blk_1073741830_1006, type=LAST_IN_PIPELINE terminating
2019-07-13 11:34:41,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-900433139-127.0.1.1-1563029088600:blk_1073741831_1007 src: /127.0.0.1:43072 dest: /127.0.0.1:50010
2019-07-13 11:34:49,631 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-900433139-127.0.1.1-1563029088600:blk_1073741832_1008 src: /127.0.0.1:43082 dest: /127.0.0.1:50010
2019-07-13 11:34:49,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43082, dest: /127.0.0.1:50010, bytes: 358, op: HDFS_WRITE, cliID: DFSClient_attempt_1563030768835_0001_r_000000_0_-1482813322_1, offset: 0, srvID: c5c8865c-7214-4b21-9137-798598342998, blockid: BP-900433139-127.0.1.1-1563029088600:blk_1073741832_1008, duration: 43128772
2019-07-13 11:34:49,677 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-900433139-127.0.1.1-1563029088600:blk_1073741832_1008, type=LAST_IN_PIPELINE terminating
2019-07-13 11:34:50,278 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43072, dest: /127.0.0.1:50010, bytes: 33346, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_492516669_1, offset: 0, srvID: c5c8865c-7214-4b21-9137-798598342998, blockid: BP-900433139-127.0.1.1-1563029088600:blk_1073741831_1007, duration: 9157717402
2019-07-13 11:34:50,279 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-900433139-127.0.1.1-1563029088600:blk_1073741831_1007, type=LAST_IN_PIPELINE terminating
2019-07-13 11:34:50,329 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-900433139-127.0.1.1-1563029088600:blk_1073741833_1009 src: /127.0.0.1:43084 dest: /127.0.0.1:50010
2019-07-13 11:34:50,344 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43084, dest: /127.0.0.1:50010, bytes: 353, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_492516669_1, offset: 0, srvID: c5c8865c-7214-4b21-9137-798598342998, blockid: BP-900433139-127.0.1.1-1563029088600:blk_1073741833_1009, duration: 3871437
2019-07-13 11:34:50,352 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-900433139-127.0.1.1-1563029088600:blk_1073741833_1009, type=LAST_IN_PIPELINE terminating
2019-07-13 11:34:50,419 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-900433139-127.0.1.1-1563029088600:blk_1073741834_1010 src: /127.0.0.1:43088 dest: /127.0.0.1:50010
2019-07-13 11:34:50,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43088, dest: /127.0.0.1:50010, bytes: 33346, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_492516669_1, offset: 0, srvID: c5c8865c-7214-4b21-9137-798598342998, blockid: BP-900433139-127.0.1.1-1563029088600:blk_1073741834_1010, duration: 20888909
2019-07-13 11:34:50,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-900433139-127.0.1.1-1563029088600:blk_1073741834_1010, type=LAST_IN_PIPELINE terminating
2019-07-13 11:34:50,499 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-900433139-127.0.1.1-1563029088600:blk_1073741835_1011 src: /127.0.0.1:43090 dest: /127.0.0.1:50010
2019-07-13 11:34:50,548 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:43090, dest: /127.0.0.1:50010, bytes: 156087, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_492516669_1, offset: 0, srvID: c5c8865c-7214-4b21-9137-798598342998, blockid: BP-900433139-127.0.1.1-1563029088600:blk_1073741835_1011, duration: 41643370
2019-07-13 11:34:50,553 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-900433139-127.0.1.1-1563029088600:blk_1073741835_1011, type=LAST_IN_PIPELINE terminating
2019-07-13 11:34:54,437 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 file /tmp/hadoop-nilesh/dfs/data/current/BP-900433139-127.0.1.1-1563029088600/current/finalized/subdir0/subdir0/blk_1073741826 for deletion
2019-07-13 11:34:54,443 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 file /tmp/hadoop-nilesh/dfs/data/current/BP-900433139-127.0.1.1-1563029088600/current/finalized/subdir0/subdir0/blk_1073741827 for deletion
2019-07-13 11:34:54,443 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 file /tmp/hadoop-nilesh/dfs/data/current/BP-900433139-127.0.1.1-1563029088600/current/finalized/subdir0/subdir0/blk_1073741828 for deletion
2019-07-13 11:34:54,443 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 file /tmp/hadoop-nilesh/dfs/data/current/BP-900433139-127.0.1.1-1563029088600/current/finalized/subdir0/subdir0/blk_1073741829 for deletion
2019-07-13 11:34:54,443 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /tmp/hadoop-nilesh/dfs/data/current/BP-900433139-127.0.1.1-1563029088600/current/finalized/subdir0/subdir0/blk_1073741830 for deletion
2019-07-13 11:34:54,443 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 file /tmp/hadoop-nilesh/dfs/data/current/BP-900433139-127.0.1.1-1563029088600/current/finalized/subdir0/subdir0/blk_1073741831 for deletion
2019-07-13 11:34:54,445 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-900433139-127.0.1.1-1563029088600 blk_1073741826_1002 file /tmp/hadoop-nilesh/dfs/data/current/BP-900433139-127.0.1.1-1563029088600/current/finalized/subdir0/subdir0/blk_1073741826
2019-07-13 11:34:54,446 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-900433139-127.0.1.1-1563029088600 blk_1073741827_1003 file /tmp/hadoop-nilesh/dfs/data/current/BP-900433139-127.0.1.1-1563029088600/current/finalized/subdir0/subdir0/blk_1073741827
2019-07-13 11:34:54,446 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-900433139-127.0.1.1-1563029088600 blk_1073741828_1004 file /tmp/hadoop-nilesh/dfs/data/current/BP-900433139-127.0.1.1-1563029088600/current/finalized/subdir0/subdir0/blk_1073741828
2019-07-13 11:34:54,446 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-900433139-127.0.1.1-1563029088600 blk_1073741829_1005 file /tmp/hadoop-nilesh/dfs/data/current/BP-900433139-127.0.1.1-1563029088600/current/finalized/subdir0/subdir0/blk_1073741829
2019-07-13 11:34:54,446 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-900433139-127.0.1.1-1563029088600 blk_1073741830_1006 file /tmp/hadoop-nilesh/dfs/data/current/BP-900433139-127.0.1.1-1563029088600/current/finalized/subdir0/subdir0/blk_1073741830
2019-07-13 11:34:54,446 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-900433139-127.0.1.1-1563029088600 blk_1073741831_1007 file /tmp/hadoop-nilesh/dfs/data/current/BP-900433139-127.0.1.1-1563029088600/current/finalized/subdir0/subdir0/blk_1073741831
2019-07-13 11:36:09,529 INFO datanode.webhdfs: 127.0.0.1 GET /webhdfs/v1/temp/part-r-00000?op=OPEN&namenoderpcaddress=localhost:9000&offset=0 200
2019-07-13 11:41:12,480 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: IOException in offerService
java.io.EOFException: End of File Exception between local host is: "ubuntu/127.0.1.1"; destination host is: "localhost":9000; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:801)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:765)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1493)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:154)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:459)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:581)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:775)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1794)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1163)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1059)
2019-07-13 11:41:16,480 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 11:41:17,484 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 11:41:18,488 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 11:41:19,490 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 11:41:19,974 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-07-13 11:41:19,982 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2019-07-13 22:43:07,642 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = nilesh
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/etc/hadoop:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-codec-1.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-lang-2.6.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/avro-1.7.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jersey-server-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-io-2.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/asm-3.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/activation-1.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-net-3.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/guava-11.0.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jsp-api-2.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jsch-0.1.54.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/xz-1.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-digester-1.8.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/junit-4.11.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/log4j-1.2.17.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-cli-1.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/servlet-api-2.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jettison-1.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jetty-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jersey-core-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/paranamer-2.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jersey-json-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/xmlenc-0.52.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/gson-2.2.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/hadoop-common-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/asm-3.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/asm-3.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/activation-1.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/javax.inject-1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/xz-1.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jettison-1.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/guice-3.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/fst-2.50.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_101
************************************************************/
2019-07-13 22:43:07,668 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-07-13 22:43:08,466 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-07-13 22:43:09,177 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-07-13 22:43:09,310 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-07-13 22:43:09,310 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-07-13 22:43:09,385 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-07-13 22:43:09,388 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2019-07-13 22:43:09,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-07-13 22:43:09,459 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-07-13 22:43:09,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-07-13 22:43:09,472 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-07-13 22:43:09,868 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-07-13 22:43:09,886 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-07-13 22:43:09,913 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-07-13 22:43:09,929 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-07-13 22:43:09,934 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-07-13 22:43:09,935 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-07-13 22:43:09,935 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-07-13 22:43:09,976 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 44453
2019-07-13 22:43:09,976 INFO org.mortbay.log: jetty-6.1.26
2019-07-13 22:43:10,297 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:44453
2019-07-13 22:43:11,493 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-07-13 22:43:11,518 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-07-13 22:43:12,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = nilesh
2019-07-13 22:43:12,204 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-07-13 22:43:12,590 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-07-13 22:43:12,717 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-07-13 22:43:13,101 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-07-13 22:43:13,211 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-07-13 22:43:13,251 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-07-13 22:43:13,346 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-07-13 22:43:13,393 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-07-13 22:43:13,401 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-07-13 22:43:14,766 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:15,768 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:16,770 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:17,771 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:18,772 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:19,772 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:20,773 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:21,774 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:22,775 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:23,776 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:23,777 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:745)
2019-07-13 22:43:23,780 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-07-13 22:43:29,783 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:30,784 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:31,785 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:32,786 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:33,787 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:34,788 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:35,798 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:36,798 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:37,800 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:38,801 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:38,801 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:745)
2019-07-13 22:43:38,802 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-07-13 22:43:44,807 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:45,808 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:46,811 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:47,813 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:48,815 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:49,819 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:50,822 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:51,824 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:52,826 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:53,828 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:43:53,830 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:745)
2019-07-13 22:43:53,831 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-07-13 22:43:59,834 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:00,836 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:01,838 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:02,840 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:03,843 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:04,846 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:05,851 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:06,854 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:07,856 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:08,859 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:08,860 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:745)
2019-07-13 22:44:08,864 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-07-13 22:44:14,867 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:15,869 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:16,871 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:17,875 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:18,877 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:19,879 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:20,887 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:21,894 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:22,896 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:23,897 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:23,898 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:745)
2019-07-13 22:44:23,900 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-07-13 22:44:29,903 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:30,905 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:31,911 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:32,913 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:33,917 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:34,919 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:35,921 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:36,923 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:37,926 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:38,929 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:44:38,930 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:745)
2019-07-13 22:44:38,931 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-07-13 22:44:39,691 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-07-13 22:44:39,696 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2019-07-13 22:45:32,447 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = nilesh
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/etc/hadoop:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-codec-1.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-lang-2.6.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/avro-1.7.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jersey-server-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-io-2.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/asm-3.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/activation-1.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-net-3.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/guava-11.0.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jsp-api-2.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jsch-0.1.54.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/xz-1.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-digester-1.8.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/junit-4.11.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/log4j-1.2.17.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-cli-1.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/servlet-api-2.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jettison-1.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jetty-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jersey-core-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/paranamer-2.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jersey-json-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/xmlenc-0.52.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/gson-2.2.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/hadoop-common-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/asm-3.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/asm-3.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/activation-1.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/javax.inject-1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/xz-1.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jettison-1.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/guice-3.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/fst-2.50.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_101
************************************************************/
2019-07-13 22:45:32,465 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-07-13 22:45:33,180 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-07-13 22:45:33,714 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-07-13 22:45:33,843 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-07-13 22:45:33,843 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-07-13 22:45:33,850 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-07-13 22:45:33,851 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2019-07-13 22:45:33,854 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-07-13 22:45:33,884 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-07-13 22:45:33,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-07-13 22:45:33,888 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-07-13 22:45:34,044 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-07-13 22:45:34,060 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-07-13 22:45:34,091 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-07-13 22:45:34,101 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-07-13 22:45:34,110 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-07-13 22:45:34,110 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-07-13 22:45:34,110 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-07-13 22:45:34,141 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 45183
2019-07-13 22:45:34,141 INFO org.mortbay.log: jetty-6.1.26
2019-07-13 22:45:34,436 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:45183
2019-07-13 22:45:34,764 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-07-13 22:45:34,778 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-07-13 22:45:35,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = nilesh
2019-07-13 22:45:35,073 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-07-13 22:45:35,218 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-07-13 22:45:35,257 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-07-13 22:45:35,357 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-07-13 22:45:35,379 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-07-13 22:45:35,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-07-13 22:45:35,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-07-13 22:45:35,446 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-07-13 22:45:35,447 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-07-13 22:45:36,621 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:45:37,622 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:45:38,625 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:45:39,627 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:45:40,629 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:45:41,631 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:45:42,631 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:45:43,632 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:45:44,633 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:45:45,636 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:45:45,638 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:745)
2019-07-13 22:45:45,656 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-07-13 22:45:51,664 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:45:52,666 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:45:53,669 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:45:54,674 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:45:55,677 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:45:56,679 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:45:57,681 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:45:58,684 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:45:59,686 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:46:00,688 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:46:00,689 WARN org.apache.hadoop.ipc.Client: Failed to connect to server: localhost/127.0.0.1:9000: retries get failed due to exceeded maximum allowed retries number: 10
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)
	at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:685)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:788)
	at org.apache.hadoop.ipc.Client$Connection.access$3500(Client.java:410)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1550)
	at org.apache.hadoop.ipc.Client.call(Client.java:1381)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy15.versionRequest(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.versionRequest(DatanodeProtocolClientSideTranslatorPB.java:274)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.retrieveNamespaceInfo(BPServiceActor.java:215)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:261)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:750)
	at java.lang.Thread.run(Thread.java:745)
2019-07-13 22:46:00,692 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Problem connecting to server: localhost/127.0.0.1:9000
2019-07-13 22:46:06,696 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2019-07-13 22:46:07,561 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: RECEIVED SIGNAL 15: SIGTERM
2019-07-13 22:46:07,564 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at ubuntu/127.0.1.1
************************************************************/
2019-07-13 22:47:05,943 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting DataNode
STARTUP_MSG:   user = nilesh
STARTUP_MSG:   host = ubuntu/127.0.1.1
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.8.5
STARTUP_MSG:   classpath = /home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/etc/hadoop:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-codec-1.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/htrace-core4-4.0.1-incubating.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/httpcore-4.4.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-lang-2.6.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/avro-1.7.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/hadoop-auth-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jersey-server-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-io-2.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/asm-3.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/activation-1.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-net-3.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/guava-11.0.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jsp-api-2.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jsch-0.1.54.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/xz-1.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-digester-1.8.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/hadoop-annotations-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/junit-4.11.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/log4j-1.2.17.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-cli-1.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/servlet-api-2.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jettison-1.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jetty-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jersey-core-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/json-smart-1.3.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/httpclient-4.5.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/paranamer-2.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/jersey-json-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/xmlenc-0.52.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/lib/gson-2.2.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/hadoop-common-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/hadoop-nfs-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/common/hadoop-common-2.8.5-tests.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/hadoop-hdfs-client-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/htrace-core4-4.0.1-incubating.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/okio-1.4.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/asm-3.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5-tests.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-2.8.5-tests.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-native-client-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-nfs-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/hdfs/hadoop-hdfs-client-2.8.5-tests.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/curator-test-2.7.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/asm-3.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/activation-1.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/javax.inject-1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/xz-1.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/json-io-2.5.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/java-util-1.9.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-math-2.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/javassist-3.18.1-GA.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jettison-1.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/guice-3.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/fst-2.50.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/curator-client-2.7.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-common-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-common-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-client-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-server-tests-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-api-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/yarn/hadoop-yarn-registry-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/hadoop-annotations-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.8.5-tests.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.8.5.jar:/home/nilesh/HADOOP/hadoop_home/hadoop-2.8.5/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.8.5.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r 0b8464d75227fcee2c6e7f2410377b3d53d3d5f8; compiled by 'jdu' on 2018-09-10T03:32Z
STARTUP_MSG:   java = 1.8.0_101
************************************************************/
2019-07-13 22:47:05,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2019-07-13 22:47:06,681 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2019-07-13 22:47:07,208 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2019-07-13 22:47:07,346 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
2019-07-13 22:47:07,346 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: DataNode metrics system started
2019-07-13 22:47:07,358 INFO org.apache.hadoop.hdfs.server.datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576
2019-07-13 22:47:07,360 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Configured hostname is ubuntu
2019-07-13 22:47:07,366 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting DataNode with maxLockedMemory = 0
2019-07-13 22:47:07,405 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened streaming server at /0.0.0.0:50010
2019-07-13 22:47:07,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Balancing bandwith is 10485760 bytes/s
2019-07-13 22:47:07,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Number threads for balancing is 50
2019-07-13 22:47:07,566 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2019-07-13 22:47:07,583 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2019-07-13 22:47:07,613 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.datanode is not defined
2019-07-13 22:47:07,621 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2019-07-13 22:47:07,628 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2019-07-13 22:47:07,628 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2019-07-13 22:47:07,628 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2019-07-13 22:47:07,652 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 44519
2019-07-13 22:47:07,652 INFO org.mortbay.log: jetty-6.1.26
2019-07-13 22:47:07,893 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:44519
2019-07-13 22:47:08,215 INFO org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075
2019-07-13 22:47:08,225 INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor
2019-07-13 22:47:08,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnUserName = nilesh
2019-07-13 22:47:08,531 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: supergroup = supergroup
2019-07-13 22:47:08,694 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2019-07-13 22:47:08,743 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 50020
2019-07-13 22:47:08,843 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Opened IPC server at /0.0.0.0:50020
2019-07-13 22:47:08,876 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Refresh request received for nameservices: null
2019-07-13 22:47:08,896 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Starting BPOfferServices for nameservices: <default>
2019-07-13 22:47:08,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000 starting to offer service
2019-07-13 22:47:08,950 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2019-07-13 22:47:08,953 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 50020: starting
2019-07-13 22:47:09,609 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:9000
2019-07-13 22:47:09,631 INFO org.apache.hadoop.hdfs.server.common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2019-07-13 22:47:09,638 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-nilesh/dfs/data/in_use.lock acquired by nodename 5699@ubuntu
2019-07-13 22:47:09,646 INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory /tmp/hadoop-nilesh/dfs/data is not formatted for namespace 1195929320. Formatting...
2019-07-13 22:47:09,647 INFO org.apache.hadoop.hdfs.server.common.Storage: Generated new storageID DS-28e8aaa8-6b57-4506-943e-28c957cbdace for directory /tmp/hadoop-nilesh/dfs/data
2019-07-13 22:47:09,955 INFO org.apache.hadoop.hdfs.server.common.Storage: Analyzing storage directories for bpid BP-1798474148-127.0.1.1-1563083204750
2019-07-13 22:47:09,955 INFO org.apache.hadoop.hdfs.server.common.Storage: Locking is disabled for /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750
2019-07-13 22:47:09,955 INFO org.apache.hadoop.hdfs.server.common.Storage: Block pool storage directory /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750 is not formatted for BP-1798474148-127.0.1.1-1563083204750. Formatting ...
2019-07-13 22:47:09,955 INFO org.apache.hadoop.hdfs.server.common.Storage: Formatting block pool BP-1798474148-127.0.1.1-1563083204750 directory /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current
2019-07-13 22:47:09,961 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Setting up storage: nsid=1195929320;bpid=BP-1798474148-127.0.1.1-1563083204750;lv=-57;nsInfo=lv=-63;cid=CID-9e2adc49-5ad3-4b32-aa02-70e677dc6877;nsid=1195929320;c=1563083204750;bpid=BP-1798474148-127.0.1.1-1563083204750;dnuuid=null
2019-07-13 22:47:09,964 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Generated and persisted new Datanode UUID 76507605-3e89-4178-8acd-669991b4c7b4
2019-07-13 22:47:10,208 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added new volume: DS-28e8aaa8-6b57-4506-943e-28c957cbdace
2019-07-13 22:47:10,208 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Added volume - /tmp/hadoop-nilesh/dfs/data/current, StorageType: DISK
2019-07-13 22:47:10,234 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Registered FSDatasetState MBean
2019-07-13 22:47:10,244 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Volume reference is released.
2019-07-13 22:47:10,245 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding block pool BP-1798474148-127.0.1.1-1563083204750
2019-07-13 22:47:10,260 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Scanning block pool BP-1798474148-127.0.1.1-1563083204750 on volume /tmp/hadoop-nilesh/dfs/data/current...
2019-07-13 22:47:10,328 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time taken to scan block pool BP-1798474148-127.0.1.1-1563083204750 on /tmp/hadoop-nilesh/dfs/data/current: 67ms
2019-07-13 22:47:10,329 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1798474148-127.0.1.1-1563083204750: 85ms
2019-07-13 22:47:10,341 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Adding replicas to map for block pool BP-1798474148-127.0.1.1-1563083204750 on volume /tmp/hadoop-nilesh/dfs/data/current...
2019-07-13 22:47:10,342 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice: Replica Cache file: /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/replicas doesn't exist 
2019-07-13 22:47:10,342 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1798474148-127.0.1.1-1563083204750 on volume /tmp/hadoop-nilesh/dfs/data/current: 0ms
2019-07-13 22:47:10,342 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl: Total time to add all replicas to map: 12ms
2019-07-13 22:47:10,348 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: Now scanning bpid BP-1798474148-127.0.1.1-1563083204750 on volume /tmp/hadoop-nilesh/dfs/data
2019-07-13 22:47:10,364 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 7/14/19 3:57 AM with interval of 21600000ms
2019-07-13 22:47:10,370 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/tmp/hadoop-nilesh/dfs/data, DS-28e8aaa8-6b57-4506-943e-28c957cbdace): finished scanning block pool BP-1798474148-127.0.1.1-1563083204750
2019-07-13 22:47:10,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool BP-1798474148-127.0.1.1-1563083204750 (Datanode Uuid 76507605-3e89-4178-8acd-669991b4c7b4) service to localhost/127.0.0.1:9000 beginning handshake with NN
2019-07-13 22:47:10,479 INFO org.apache.hadoop.hdfs.server.datanode.VolumeScanner: VolumeScanner(/tmp/hadoop-nilesh/dfs/data, DS-28e8aaa8-6b57-4506-943e-28c957cbdace): no suitable block pools found to scan.  Waiting 1814399865 ms.
2019-07-13 22:47:10,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Block pool Block pool BP-1798474148-127.0.1.1-1563083204750 (Datanode Uuid 76507605-3e89-4178-8acd-669991b4c7b4) service to localhost/127.0.0.1:9000 successfully registered with NN
2019-07-13 22:47:10,576 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: For namenode localhost/127.0.0.1:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2019-07-13 22:47:10,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x10f0d5f9ab8898b1,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 26 msec to generate and 122 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-07-13 22:47:10,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1798474148-127.0.1.1-1563083204750
2019-07-13 23:15:19,763 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1876ms
No GCs detected
2019-07-13 23:15:22,386 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2122ms
No GCs detected
2019-07-13 23:15:25,006 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1618ms
No GCs detected
2019-07-13 23:15:27,644 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2132ms
No GCs detected
2019-07-13 23:15:30,225 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2080ms
No GCs detected
2019-07-13 23:57:34,618 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741825_1001 src: /127.0.0.1:59496 dest: /127.0.0.1:50010
2019-07-13 23:57:34,723 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59496, dest: /127.0.0.1:50010, bytes: 1096, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1811209231_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741825_1001, duration: 17239976
2019-07-13 23:57:34,739 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
2019-07-13 23:59:57,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741826_1002 src: /127.0.0.1:59516 dest: /127.0.0.1:50010
2019-07-13 23:59:57,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59516, dest: /127.0.0.1:50010, bytes: 4887, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_227262412_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741826_1002, duration: 15388681
2019-07-13 23:59:57,849 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating
2019-07-13 23:59:58,028 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741827_1003 src: /127.0.0.1:59518 dest: /127.0.0.1:50010
2019-07-13 23:59:58,041 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59518, dest: /127.0.0.1:50010, bytes: 110, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_227262412_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741827_1003, duration: 5947860
2019-07-13 23:59:58,050 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating
2019-07-13 23:59:58,060 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741828_1004 src: /127.0.0.1:59520 dest: /127.0.0.1:50010
2019-07-13 23:59:58,069 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59520, dest: /127.0.0.1:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_227262412_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741828_1004, duration: 3322967
2019-07-13 23:59:58,074 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741828_1004, type=LAST_IN_PIPELINE terminating
2019-07-13 23:59:58,257 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741829_1005 src: /127.0.0.1:59522 dest: /127.0.0.1:50010
2019-07-13 23:59:58,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59522, dest: /127.0.0.1:50010, bytes: 134214, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_227262412_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741829_1005, duration: 32254740
2019-07-13 23:59:58,293 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741829_1005, type=LAST_IN_PIPELINE terminating
2019-07-14 00:00:11,380 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741830_1006 src: /127.0.0.1:59538 dest: /127.0.0.1:50010
2019-07-14 00:00:11,402 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59538, dest: /127.0.0.1:50010, bytes: 156101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2106897882_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741830_1006, duration: 18260711
2019-07-14 00:00:11,410 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741830_1006, type=LAST_IN_PIPELINE terminating
2019-07-14 00:00:20,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741831_1007 src: /127.0.0.1:59550 dest: /127.0.0.1:50010
2019-07-14 00:00:38,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59550, dest: /127.0.0.1:50010, bytes: 21183, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2106897882_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741831_1007, duration: 17810423082
2019-07-14 00:00:38,342 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741831_1007, type=LAST_IN_PIPELINE terminating
2019-07-14 00:00:38,378 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741832_1008 src: /127.0.0.1:59584 dest: /127.0.0.1:50010
2019-07-14 00:00:38,392 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59584, dest: /127.0.0.1:50010, bytes: 340, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2106897882_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741832_1008, duration: 11282151
2019-07-14 00:00:38,393 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741832_1008, type=LAST_IN_PIPELINE terminating
2019-07-14 00:00:38,483 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741833_1009 src: /127.0.0.1:59588 dest: /127.0.0.1:50010
2019-07-14 00:00:38,494 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59588, dest: /127.0.0.1:50010, bytes: 21183, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2106897882_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741833_1009, duration: 8328960
2019-07-14 00:00:38,494 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741833_1009, type=LAST_IN_PIPELINE terminating
2019-07-14 00:00:38,542 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741834_1010 src: /127.0.0.1:59590 dest: /127.0.0.1:50010
2019-07-14 00:00:38,574 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59590, dest: /127.0.0.1:50010, bytes: 156101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-2106897882_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741834_1010, duration: 26542323
2019-07-14 00:00:38,582 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741834_1010, type=LAST_IN_PIPELINE terminating
2019-07-14 00:00:40,304 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741826 for deletion
2019-07-14 00:00:40,309 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741827 for deletion
2019-07-14 00:00:40,309 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741828 for deletion
2019-07-14 00:00:40,309 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741829 for deletion
2019-07-14 00:00:40,309 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741830 for deletion
2019-07-14 00:00:40,310 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741831 for deletion
2019-07-14 00:00:40,310 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741826_1002 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741826
2019-07-14 00:00:40,310 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741827_1003 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741827
2019-07-14 00:00:40,311 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741828_1004 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741828
2019-07-14 00:00:40,311 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741829_1005 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741829
2019-07-14 00:00:40,311 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741830_1006 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741830
2019-07-14 00:00:40,311 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741831_1007 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741831
2019-07-14 00:04:49,566 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741835_1011 src: /127.0.0.1:59614 dest: /127.0.0.1:50010
2019-07-14 00:04:49,601 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59614, dest: /127.0.0.1:50010, bytes: 4886, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_733676513_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741835_1011, duration: 4729960
2019-07-14 00:04:49,610 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741835_1011, type=LAST_IN_PIPELINE terminating
2019-07-14 00:04:49,715 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741836_1012 src: /127.0.0.1:59616 dest: /127.0.0.1:50010
2019-07-14 00:04:49,730 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59616, dest: /127.0.0.1:50010, bytes: 110, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_733676513_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741836_1012, duration: 7588813
2019-07-14 00:04:49,740 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741836_1012, type=LAST_IN_PIPELINE terminating
2019-07-14 00:04:49,750 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741837_1013 src: /127.0.0.1:59618 dest: /127.0.0.1:50010
2019-07-14 00:04:49,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59618, dest: /127.0.0.1:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_733676513_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741837_1013, duration: 5231378
2019-07-14 00:04:49,758 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741837_1013, type=LAST_IN_PIPELINE terminating
2019-07-14 00:04:49,933 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741838_1014 src: /127.0.0.1:59620 dest: /127.0.0.1:50010
2019-07-14 00:04:49,969 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59620, dest: /127.0.0.1:50010, bytes: 134214, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_733676513_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741838_1014, duration: 34618884
2019-07-14 00:04:49,980 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741838_1014, type=LAST_IN_PIPELINE terminating
2019-07-14 00:04:57,511 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741839_1015 src: /127.0.0.1:59636 dest: /127.0.0.1:50010
2019-07-14 00:04:57,540 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59636, dest: /127.0.0.1:50010, bytes: 156101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2021220818_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741839_1015, duration: 23566872
2019-07-14 00:04:57,545 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741839_1015, type=LAST_IN_PIPELINE terminating
2019-07-14 00:05:04,299 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741840_1016 src: /127.0.0.1:59648 dest: /127.0.0.1:50010
2019-07-14 00:05:21,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59648, dest: /127.0.0.1:50010, bytes: 21183, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2021220818_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741840_1016, duration: 17062520384
2019-07-14 00:05:21,363 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741840_1016, type=LAST_IN_PIPELINE terminating
2019-07-14 00:05:21,387 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741841_1017 src: /127.0.0.1:59678 dest: /127.0.0.1:50010
2019-07-14 00:05:21,401 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59678, dest: /127.0.0.1:50010, bytes: 340, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2021220818_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741841_1017, duration: 10516793
2019-07-14 00:05:21,401 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741841_1017, type=LAST_IN_PIPELINE terminating
2019-07-14 00:05:21,485 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741842_1018 src: /127.0.0.1:59682 dest: /127.0.0.1:50010
2019-07-14 00:05:21,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59682, dest: /127.0.0.1:50010, bytes: 21183, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2021220818_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741842_1018, duration: 9542555
2019-07-14 00:05:21,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741842_1018, type=LAST_IN_PIPELINE terminating
2019-07-14 00:05:21,549 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741843_1019 src: /127.0.0.1:59684 dest: /127.0.0.1:50010
2019-07-14 00:05:21,575 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59684, dest: /127.0.0.1:50010, bytes: 156101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2021220818_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741843_1019, duration: 19617637
2019-07-14 00:05:21,579 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741843_1019, type=LAST_IN_PIPELINE terminating
2019-07-14 00:05:25,355 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741840_1016 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741840 for deletion
2019-07-14 00:05:25,356 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741835_1011 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741835 for deletion
2019-07-14 00:05:25,356 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741836_1012 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741836 for deletion
2019-07-14 00:05:25,356 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741837_1013 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741837 for deletion
2019-07-14 00:05:25,356 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741838_1014 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741838 for deletion
2019-07-14 00:05:25,356 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741839_1015 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741839 for deletion
2019-07-14 00:05:25,357 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741840_1016 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741840
2019-07-14 00:05:25,357 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741835_1011 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741835
2019-07-14 00:05:25,357 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741836_1012 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741836
2019-07-14 00:05:25,357 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741837_1013 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741837
2019-07-14 00:05:25,357 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741838_1014 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741838
2019-07-14 00:05:25,359 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741839_1015 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741839
2019-07-14 00:08:14,422 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741844_1020 src: /127.0.0.1:59702 dest: /127.0.0.1:50010
2019-07-14 00:08:14,448 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59702, dest: /127.0.0.1:50010, bytes: 4881, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-630170005_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741844_1020, duration: 21596052
2019-07-14 00:08:14,449 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741844_1020, type=LAST_IN_PIPELINE terminating
2019-07-14 00:08:14,600 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741845_1021 src: /127.0.0.1:59704 dest: /127.0.0.1:50010
2019-07-14 00:08:14,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59704, dest: /127.0.0.1:50010, bytes: 110, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-630170005_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741845_1021, duration: 12100902
2019-07-14 00:08:14,627 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741845_1021, type=LAST_IN_PIPELINE terminating
2019-07-14 00:08:14,667 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741846_1022 src: /127.0.0.1:59706 dest: /127.0.0.1:50010
2019-07-14 00:08:14,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59706, dest: /127.0.0.1:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-630170005_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741846_1022, duration: 9127672
2019-07-14 00:08:14,678 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741846_1022, type=LAST_IN_PIPELINE terminating
2019-07-14 00:08:14,937 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741847_1023 src: /127.0.0.1:59708 dest: /127.0.0.1:50010
2019-07-14 00:08:14,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59708, dest: /127.0.0.1:50010, bytes: 134214, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-630170005_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741847_1023, duration: 29381486
2019-07-14 00:08:14,968 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741847_1023, type=LAST_IN_PIPELINE terminating
2019-07-14 00:08:27,085 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741848_1024 src: /127.0.0.1:59730 dest: /127.0.0.1:50010
2019-07-14 00:08:27,112 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59730, dest: /127.0.0.1:50010, bytes: 156101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-769663446_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741848_1024, duration: 24628553
2019-07-14 00:08:27,119 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741848_1024, type=LAST_IN_PIPELINE terminating
2019-07-14 00:08:34,146 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741849_1025 src: /127.0.0.1:59742 dest: /127.0.0.1:50010
2019-07-14 00:08:54,749 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59742, dest: /127.0.0.1:50010, bytes: 21183, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-769663446_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741849_1025, duration: 20587794929
2019-07-14 00:08:54,751 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741849_1025, type=LAST_IN_PIPELINE terminating
2019-07-14 00:08:54,809 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741850_1026 src: /127.0.0.1:59770 dest: /127.0.0.1:50010
2019-07-14 00:08:54,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59770, dest: /127.0.0.1:50010, bytes: 340, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-769663446_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741850_1026, duration: 12534120
2019-07-14 00:08:54,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741850_1026, type=LAST_IN_PIPELINE terminating
2019-07-14 00:08:54,936 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741851_1027 src: /127.0.0.1:59774 dest: /127.0.0.1:50010
2019-07-14 00:08:54,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59774, dest: /127.0.0.1:50010, bytes: 21183, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-769663446_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741851_1027, duration: 15328259
2019-07-14 00:08:54,954 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741851_1027, type=LAST_IN_PIPELINE terminating
2019-07-14 00:08:55,019 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741852_1028 src: /127.0.0.1:59776 dest: /127.0.0.1:50010
2019-07-14 00:08:55,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59776, dest: /127.0.0.1:50010, bytes: 156101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-769663446_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741852_1028, duration: 39789039
2019-07-14 00:08:55,061 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741852_1028, type=LAST_IN_PIPELINE terminating
2019-07-14 00:08:58,398 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741844_1020 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741844 for deletion
2019-07-14 00:08:58,400 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741845_1021 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741845 for deletion
2019-07-14 00:08:58,400 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741846_1022 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741846 for deletion
2019-07-14 00:08:58,400 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741847_1023 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741847 for deletion
2019-07-14 00:08:58,401 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741848_1024 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741848 for deletion
2019-07-14 00:08:58,401 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741849_1025 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741849 for deletion
2019-07-14 00:08:58,409 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741844_1020 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741844
2019-07-14 00:08:58,409 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741845_1021 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741845
2019-07-14 00:08:58,409 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741846_1022 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741846
2019-07-14 00:08:58,409 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741847_1023 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741847
2019-07-14 00:08:58,412 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741848_1024 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741848
2019-07-14 00:08:58,413 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741849_1025 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741849
2019-07-14 00:16:12,139 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741853_1029 src: /127.0.0.1:59816 dest: /127.0.0.1:50010
2019-07-14 00:16:12,168 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59816, dest: /127.0.0.1:50010, bytes: 4881, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_861169772_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741853_1029, duration: 23638075
2019-07-14 00:16:12,168 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741853_1029, type=LAST_IN_PIPELINE terminating
2019-07-14 00:16:12,316 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741854_1030 src: /127.0.0.1:59818 dest: /127.0.0.1:50010
2019-07-14 00:16:12,328 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59818, dest: /127.0.0.1:50010, bytes: 110, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_861169772_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741854_1030, duration: 10006151
2019-07-14 00:16:12,328 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741854_1030, type=LAST_IN_PIPELINE terminating
2019-07-14 00:16:12,359 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741855_1031 src: /127.0.0.1:59820 dest: /127.0.0.1:50010
2019-07-14 00:16:12,384 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59820, dest: /127.0.0.1:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_861169772_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741855_1031, duration: 23205766
2019-07-14 00:16:12,385 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741855_1031, type=LAST_IN_PIPELINE terminating
2019-07-14 00:16:12,600 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741856_1032 src: /127.0.0.1:59822 dest: /127.0.0.1:50010
2019-07-14 00:16:12,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59822, dest: /127.0.0.1:50010, bytes: 134214, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_861169772_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741856_1032, duration: 41165199
2019-07-14 00:16:12,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741856_1032, type=LAST_IN_PIPELINE terminating
2019-07-14 00:16:22,193 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741857_1033 src: /127.0.0.1:59840 dest: /127.0.0.1:50010
2019-07-14 00:16:22,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59840, dest: /127.0.0.1:50010, bytes: 156101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-386837778_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741857_1033, duration: 26040272
2019-07-14 00:16:22,221 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741857_1033, type=LAST_IN_PIPELINE terminating
2019-07-14 00:16:29,051 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741858_1034 src: /127.0.0.1:59852 dest: /127.0.0.1:50010
2019-07-14 00:16:48,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59852, dest: /127.0.0.1:50010, bytes: 21183, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-386837778_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741858_1034, duration: 19870713728
2019-07-14 00:16:48,923 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741858_1034, type=LAST_IN_PIPELINE terminating
2019-07-14 00:16:48,967 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741859_1035 src: /127.0.0.1:59880 dest: /127.0.0.1:50010
2019-07-14 00:16:48,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59880, dest: /127.0.0.1:50010, bytes: 340, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-386837778_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741859_1035, duration: 9333445
2019-07-14 00:16:48,978 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741859_1035, type=LAST_IN_PIPELINE terminating
2019-07-14 00:16:49,075 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741860_1036 src: /127.0.0.1:59884 dest: /127.0.0.1:50010
2019-07-14 00:16:49,091 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59884, dest: /127.0.0.1:50010, bytes: 21183, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-386837778_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741860_1036, duration: 14124863
2019-07-14 00:16:49,092 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741860_1036, type=LAST_IN_PIPELINE terminating
2019-07-14 00:16:49,154 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741861_1037 src: /127.0.0.1:59886 dest: /127.0.0.1:50010
2019-07-14 00:16:49,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:59886, dest: /127.0.0.1:50010, bytes: 156101, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-386837778_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741861_1037, duration: 30275952
2019-07-14 00:16:49,197 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741861_1037, type=LAST_IN_PIPELINE terminating
2019-07-14 00:16:55,498 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741856_1032 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741856 for deletion
2019-07-14 00:16:55,499 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741857_1033 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741857 for deletion
2019-07-14 00:16:55,499 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741858_1034 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741858 for deletion
2019-07-14 00:16:55,499 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741853_1029 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741853 for deletion
2019-07-14 00:16:55,499 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741854_1030 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741854 for deletion
2019-07-14 00:16:55,499 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741855_1031 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741855 for deletion
2019-07-14 00:16:55,500 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741856_1032 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741856
2019-07-14 00:16:55,501 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741857_1033 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741857
2019-07-14 00:16:55,501 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741858_1034 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741858
2019-07-14 00:16:55,501 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741853_1029 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741853
2019-07-14 00:16:55,504 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741854_1030 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741854
2019-07-14 00:16:55,504 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741855_1031 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741855
2019-07-14 01:38:21,776 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1715ms
No GCs detected
2019-07-14 01:57:01,897 WARN org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 23149ms
No GCs detected
2019-07-15 22:46:31,892 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741862_1038 src: /127.0.0.1:60336 dest: /127.0.0.1:50010
2019-07-15 22:46:31,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:60336, dest: /127.0.0.1:50010, bytes: 1095, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1363011792_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741862_1038, duration: 10771322
2019-07-15 22:46:31,905 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741862_1038, type=LAST_IN_PIPELINE terminating
2019-07-15 23:00:54,137 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741863_1039 src: /127.0.0.1:60388 dest: /127.0.0.1:50010
2019-07-15 23:00:54,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:60388, dest: /127.0.0.1:50010, bytes: 4984, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1156248773_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741863_1039, duration: 11124601
2019-07-15 23:00:54,152 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741863_1039, type=LAST_IN_PIPELINE terminating
2019-07-15 23:00:54,228 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741864_1040 src: /127.0.0.1:60390 dest: /127.0.0.1:50010
2019-07-15 23:00:54,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:60390, dest: /127.0.0.1:50010, bytes: 109, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1156248773_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741864_1040, duration: 3334919
2019-07-15 23:00:54,235 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741864_1040, type=LAST_IN_PIPELINE terminating
2019-07-15 23:00:54,255 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741865_1041 src: /127.0.0.1:60392 dest: /127.0.0.1:50010
2019-07-15 23:00:54,264 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:60392, dest: /127.0.0.1:50010, bytes: 22, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1156248773_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741865_1041, duration: 4710929
2019-07-15 23:00:54,265 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741865_1041, type=LAST_IN_PIPELINE terminating
2019-07-15 23:00:54,403 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741866_1042 src: /127.0.0.1:60394 dest: /127.0.0.1:50010
2019-07-15 23:00:54,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:60394, dest: /127.0.0.1:50010, bytes: 134208, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1156248773_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741866_1042, duration: 11100701
2019-07-15 23:00:54,430 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741866_1042, type=LAST_IN_PIPELINE terminating
2019-07-15 23:01:00,903 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741867_1043 src: /127.0.0.1:60412 dest: /127.0.0.1:50010
2019-07-15 23:01:00,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:60412, dest: /127.0.0.1:50010, bytes: 156095, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-338130895_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741867_1043, duration: 24675357
2019-07-15 23:01:00,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741867_1043, type=LAST_IN_PIPELINE terminating
2019-07-15 23:01:06,407 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741868_1044 src: /127.0.0.1:60422 dest: /127.0.0.1:50010
2019-07-15 23:01:12,001 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741869_1045 src: /127.0.0.1:60432 dest: /127.0.0.1:50010
2019-07-15 23:01:12,033 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:60432, dest: /127.0.0.1:50010, bytes: 3, op: HDFS_WRITE, cliID: DFSClient_attempt_1563083242560_0005_r_000000_0_22897022_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741869_1045, duration: 25566787
2019-07-15 23:01:12,038 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741869_1045, type=LAST_IN_PIPELINE terminating
2019-07-15 23:01:12,417 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:60422, dest: /127.0.0.1:50010, bytes: 33283, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-338130895_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741868_1044, duration: 6008234514
2019-07-15 23:01:12,418 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741868_1044, type=LAST_IN_PIPELINE terminating
2019-07-15 23:01:12,444 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741870_1046 src: /127.0.0.1:60434 dest: /127.0.0.1:50010
2019-07-15 23:01:12,456 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:60434, dest: /127.0.0.1:50010, bytes: 351, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-338130895_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741870_1046, duration: 9025112
2019-07-15 23:01:12,457 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741870_1046, type=LAST_IN_PIPELINE terminating
2019-07-15 23:01:12,510 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741871_1047 src: /127.0.0.1:60438 dest: /127.0.0.1:50010
2019-07-15 23:01:12,527 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:60438, dest: /127.0.0.1:50010, bytes: 33283, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-338130895_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741871_1047, duration: 8479308
2019-07-15 23:01:12,533 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741871_1047, type=LAST_IN_PIPELINE terminating
2019-07-15 23:01:12,561 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741872_1048 src: /127.0.0.1:60440 dest: /127.0.0.1:50010
2019-07-15 23:01:12,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:60440, dest: /127.0.0.1:50010, bytes: 156095, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-338130895_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741872_1048, duration: 15414768
2019-07-15 23:01:12,578 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741872_1048, type=LAST_IN_PIPELINE terminating
2019-07-15 23:01:18,435 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741863_1039 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741863 for deletion
2019-07-15 23:01:18,441 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741864_1040 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741864 for deletion
2019-07-15 23:01:18,441 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741865_1041 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741865 for deletion
2019-07-15 23:01:18,441 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741866_1042 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741866 for deletion
2019-07-15 23:01:18,441 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741867_1043 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741867 for deletion
2019-07-15 23:01:18,441 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741868_1044 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741868 for deletion
2019-07-15 23:01:18,441 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741863_1039 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741863
2019-07-15 23:01:18,441 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741864_1040 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741864
2019-07-15 23:01:18,441 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741865_1041 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741865
2019-07-15 23:01:18,442 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741866_1042 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741866
2019-07-15 23:01:18,442 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741867_1043 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741867
2019-07-15 23:01:18,442 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741868_1044 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741868
2019-07-15 23:01:37,350 INFO datanode.webhdfs: 127.0.0.1 GET /webhdfs/v1/temp/part-r-00000?op=OPEN&namenoderpcaddress=localhost:9000&offset=0 200
2019-07-15 23:01:43,948 INFO datanode.webhdfs: 127.0.0.1 GET /webhdfs/v1/temp/part-r-00000?op=OPEN&namenoderpcaddress=localhost:9000&offset=0 200
2019-07-15 23:41:28,144 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x10f0d5f9ab8898b2,  containing 1 storage report(s), of which we sent 1. The reports had 18 total blocks and used 1 RPC(s). This took 4 msec to generate and 14 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-07-15 23:41:28,145 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1798474148-127.0.1.1-1563083204750
2019-07-16 01:17:32,187 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1120ms
No GCs detected
2019-07-16 07:35:35,878 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1798474148-127.0.1.1-1563083204750 Total blocks: 18, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2019-07-16 22:08:57,617 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1080ms
No GCs detected
2019-07-16 22:10:12,988 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2385ms
No GCs detected
2019-07-16 22:11:38,137 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1530ms
No GCs detected
2019-07-16 22:11:57,221 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1555ms
No GCs detected
2019-07-16 22:23:26,509 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741873_1049 src: /127.0.0.1:34660 dest: /127.0.0.1:50010
2019-07-16 22:23:26,534 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:34660, dest: /127.0.0.1:50010, bytes: 10, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-6940134_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741873_1049, duration: 18022036
2019-07-16 22:23:26,540 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741873_1049, type=LAST_IN_PIPELINE terminating
2019-07-17 00:23:46,985 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Successfully sent block report 0x10f0d5f9ab8898b3,  containing 1 storage report(s), of which we sent 1. The reports had 19 total blocks and used 1 RPC(s). This took 18 msec to generate and 6 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2019-07-17 00:23:46,987 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Got finalize command for block pool BP-1798474148-127.0.1.1-1563083204750
2019-07-17 04:17:07,537 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1762ms
No GCs detected
2019-07-17 06:41:07,095 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1399ms
No GCs detected
2019-07-17 07:01:06,374 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2373ms
No GCs detected
2019-07-17 07:01:11,996 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2089ms
No GCs detected
2019-07-17 07:05:11,316 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1061ms
No GCs detected
2019-07-17 07:11:51,842 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1671ms
No GCs detected
2019-07-17 07:12:18,717 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 5803ms
No GCs detected
2019-07-17 07:53:14,114 INFO org.apache.hadoop.hdfs.server.datanode.DirectoryScanner: BlockPool BP-1798474148-127.0.1.1-1563083204750 Total blocks: 19, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0
2019-07-17 07:53:37,213 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741874_1050 src: /127.0.0.1:37738 dest: /127.0.0.1:50010
2019-07-17 07:53:37,336 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37738, dest: /127.0.0.1:50010, bytes: 89, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_857636639_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741874_1050, duration: 65645943
2019-07-17 07:53:37,341 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741874_1050, type=LAST_IN_PIPELINE terminating
2019-07-17 07:53:42,426 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741875_1051 src: /127.0.0.1:37740 dest: /127.0.0.1:50010
2019-07-17 07:53:42,496 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37740, dest: /127.0.0.1:50010, bytes: 4098, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_857636639_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741875_1051, duration: 13374708
2019-07-17 07:53:42,502 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741875_1051, type=LAST_IN_PIPELINE terminating
2019-07-17 07:53:45,630 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741876_1052 src: /127.0.0.1:37744 dest: /127.0.0.1:50010
2019-07-17 07:53:46,649 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37744, dest: /127.0.0.1:50010, bytes: 34207122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_857636639_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741876_1052, duration: 1017635283
2019-07-17 07:53:46,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741876_1052, type=LAST_IN_PIPELINE terminating
2019-07-17 07:53:47,226 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741877_1053 src: /127.0.0.1:37746 dest: /127.0.0.1:50010
2019-07-17 07:53:47,236 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37746, dest: /127.0.0.1:50010, bytes: 281, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_857636639_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741877_1053, duration: 6139161
2019-07-17 07:53:47,236 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741877_1053, type=LAST_IN_PIPELINE terminating
2019-07-17 07:53:47,279 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741878_1054 src: /127.0.0.1:37748 dest: /127.0.0.1:50010
2019-07-17 07:53:47,298 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37748, dest: /127.0.0.1:50010, bytes: 20, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_857636639_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741878_1054, duration: 10747219
2019-07-17 07:53:47,300 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741878_1054, type=LAST_IN_PIPELINE terminating
2019-07-17 07:53:47,437 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741879_1055 src: /127.0.0.1:37750 dest: /127.0.0.1:50010
2019-07-17 07:53:47,519 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37750, dest: /127.0.0.1:50010, bytes: 311989, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_857636639_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741879_1055, duration: 70003546
2019-07-17 07:53:47,521 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741879_1055, type=LAST_IN_PIPELINE terminating
2019-07-17 07:55:55,784 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741875_1051 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741875 for deletion
2019-07-17 07:55:55,788 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741875_1051 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741875
2019-07-17 07:56:14,180 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741880_1056 src: /127.0.0.1:37756 dest: /127.0.0.1:50010
2019-07-17 07:56:14,194 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37756, dest: /127.0.0.1:50010, bytes: 89, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_857636639_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741880_1056, duration: 1410222
2019-07-17 07:56:14,195 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741880_1056, type=LAST_IN_PIPELINE terminating
2019-07-17 07:56:14,815 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741881_1057 src: /127.0.0.1:37758 dest: /127.0.0.1:50010
2019-07-17 07:56:14,826 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37758, dest: /127.0.0.1:50010, bytes: 4113, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_857636639_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741881_1057, duration: 2388945
2019-07-17 07:56:14,829 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741881_1057, type=LAST_IN_PIPELINE terminating
2019-07-17 07:56:15,157 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741882_1058 src: /127.0.0.1:37762 dest: /127.0.0.1:50010
2019-07-17 07:56:15,762 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37762, dest: /127.0.0.1:50010, bytes: 34207122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_857636639_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741882_1058, duration: 593615198
2019-07-17 07:56:15,762 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741882_1058, type=LAST_IN_PIPELINE terminating
2019-07-17 07:56:15,858 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741883_1059 src: /127.0.0.1:37764 dest: /127.0.0.1:50010
2019-07-17 07:56:15,875 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37764, dest: /127.0.0.1:50010, bytes: 281, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_857636639_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741883_1059, duration: 9574557
2019-07-17 07:56:15,875 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741883_1059, type=LAST_IN_PIPELINE terminating
2019-07-17 07:56:15,919 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741884_1060 src: /127.0.0.1:37766 dest: /127.0.0.1:50010
2019-07-17 07:56:15,930 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37766, dest: /127.0.0.1:50010, bytes: 20, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_857636639_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741884_1060, duration: 2474394
2019-07-17 07:56:15,933 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741884_1060, type=LAST_IN_PIPELINE terminating
2019-07-17 07:56:15,994 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741885_1061 src: /127.0.0.1:37768 dest: /127.0.0.1:50010
2019-07-17 07:56:16,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37768, dest: /127.0.0.1:50010, bytes: 311989, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_857636639_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741885_1061, duration: 29812988
2019-07-17 07:56:16,026 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741885_1061, type=LAST_IN_PIPELINE terminating
2019-07-17 07:56:43,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741881_1057 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741881 for deletion
2019-07-17 07:56:43,789 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741881_1057 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741881
2019-07-17 07:57:56,559 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741886_1062 src: /127.0.0.1:37774 dest: /127.0.0.1:50010
2019-07-17 07:57:56,569 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37774, dest: /127.0.0.1:50010, bytes: 13, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_857636639_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741886_1062, duration: 6099486
2019-07-17 07:57:56,571 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741886_1062, type=LAST_IN_PIPELINE terminating
2019-07-17 07:57:57,573 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741887_1063 src: /127.0.0.1:37776 dest: /127.0.0.1:50010
2019-07-17 07:57:57,597 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37776, dest: /127.0.0.1:50010, bytes: 4113, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_857636639_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741887_1063, duration: 1596954
2019-07-17 07:57:57,602 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741887_1063, type=LAST_IN_PIPELINE terminating
2019-07-17 07:57:57,847 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741888_1064 src: /127.0.0.1:37780 dest: /127.0.0.1:50010
2019-07-17 07:57:58,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37780, dest: /127.0.0.1:50010, bytes: 34207122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_857636639_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741888_1064, duration: 649117982
2019-07-17 07:57:58,497 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741888_1064, type=LAST_IN_PIPELINE terminating
2019-07-17 07:57:58,975 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741889_1065 src: /127.0.0.1:37782 dest: /127.0.0.1:50010
2019-07-17 07:57:58,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37782, dest: /127.0.0.1:50010, bytes: 281, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_857636639_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741889_1065, duration: 7234398
2019-07-17 07:57:58,984 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741889_1065, type=LAST_IN_PIPELINE terminating
2019-07-17 07:57:59,008 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741890_1066 src: /127.0.0.1:37784 dest: /127.0.0.1:50010
2019-07-17 07:57:59,014 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37784, dest: /127.0.0.1:50010, bytes: 20, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_857636639_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741890_1066, duration: 2940417
2019-07-17 07:57:59,015 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741890_1066, type=LAST_IN_PIPELINE terminating
2019-07-17 07:57:59,083 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741891_1067 src: /127.0.0.1:37786 dest: /127.0.0.1:50010
2019-07-17 07:57:59,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:37786, dest: /127.0.0.1:50010, bytes: 311753, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_857636639_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741891_1067, duration: 27045828
2019-07-17 07:57:59,114 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741891_1067, type=LAST_IN_PIPELINE terminating
2019-07-17 08:30:04,206 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1010ms
No GCs detected
2019-07-17 09:17:42,425 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1632ms
No GCs detected
2019-07-17 10:21:50,553 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1427ms
No GCs detected
2019-07-17 10:30:56,522 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1255ms
No GCs detected
2019-07-17 10:38:25,764 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 2215ms
No GCs detected
2019-07-18 11:13:31,851 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1506ms
No GCs detected
2019-07-18 21:52:42,123 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1647ms
No GCs detected
2019-07-18 21:54:17,319 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741887_1063 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741887 for deletion
2019-07-18 21:54:17,320 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741887_1063 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741887
2019-07-18 21:57:05,708 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741892_1068 src: /127.0.0.1:38004 dest: /127.0.0.1:50010
2019-07-18 21:57:05,725 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38004, dest: /127.0.0.1:50010, bytes: 10, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_857636639_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741892_1068, duration: 2771916
2019-07-18 21:57:05,728 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741892_1068, type=LAST_IN_PIPELINE terminating
2019-07-18 21:57:06,596 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741893_1069 src: /127.0.0.1:38006 dest: /127.0.0.1:50010
2019-07-18 21:57:06,600 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38006, dest: /127.0.0.1:50010, bytes: 3902, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_857636639_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741893_1069, duration: 1072669
2019-07-18 21:57:06,603 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741893_1069, type=LAST_IN_PIPELINE terminating
2019-07-18 21:57:07,325 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741894_1070 src: /127.0.0.1:38010 dest: /127.0.0.1:50010
2019-07-18 21:57:08,166 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38010, dest: /127.0.0.1:50010, bytes: 34207122, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_857636639_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741894_1070, duration: 839378778
2019-07-18 21:57:08,172 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741894_1070, type=LAST_IN_PIPELINE terminating
2019-07-18 21:57:08,644 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741895_1071 src: /127.0.0.1:38012 dest: /127.0.0.1:50010
2019-07-18 21:57:08,651 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38012, dest: /127.0.0.1:50010, bytes: 281, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_857636639_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741895_1071, duration: 5403387
2019-07-18 21:57:08,652 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741895_1071, type=LAST_IN_PIPELINE terminating
2019-07-18 21:57:08,685 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741896_1072 src: /127.0.0.1:38014 dest: /127.0.0.1:50010
2019-07-18 21:57:08,695 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38014, dest: /127.0.0.1:50010, bytes: 20, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_857636639_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741896_1072, duration: 7541941
2019-07-18 21:57:08,695 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741896_1072, type=LAST_IN_PIPELINE terminating
2019-07-18 21:57:08,778 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741897_1073 src: /127.0.0.1:38016 dest: /127.0.0.1:50010
2019-07-18 21:57:08,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38016, dest: /127.0.0.1:50010, bytes: 311704, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_857636639_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741897_1073, duration: 20217456
2019-07-18 21:57:08,802 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741897_1073, type=LAST_IN_PIPELINE terminating
2019-07-18 22:08:48,557 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741898_1074 src: /127.0.0.1:38082 dest: /127.0.0.1:50010
2019-07-18 22:08:48,863 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38082, dest: /127.0.0.1:50010, bytes: 1099915, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2044258425_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741898_1074, duration: 304443256
2019-07-18 22:08:48,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741898_1074, type=LAST_IN_PIPELINE terminating
2019-07-18 22:09:24,097 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741899_1075 src: /127.0.0.1:38086 dest: /127.0.0.1:50010
2019-07-18 22:09:24,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38086, dest: /127.0.0.1:50010, bytes: 1099915, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2044258425_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741899_1075, duration: 120620512
2019-07-18 22:09:24,220 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741899_1075, type=LAST_IN_PIPELINE terminating
2019-07-18 22:10:22,909 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741900_1076 src: /127.0.0.1:38090 dest: /127.0.0.1:50010
2019-07-18 22:10:23,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38090, dest: /127.0.0.1:50010, bytes: 1099915, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2044258425_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741900_1076, duration: 134557264
2019-07-18 22:10:23,045 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741900_1076, type=LAST_IN_PIPELINE terminating
2019-07-18 22:12:19,738 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741901_1077 src: /127.0.0.1:38100 dest: /127.0.0.1:50010
2019-07-18 22:12:19,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38100, dest: /127.0.0.1:50010, bytes: 1099915, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2044258425_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741901_1077, duration: 128399783
2019-07-18 22:12:19,868 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741901_1077, type=LAST_IN_PIPELINE terminating
2019-07-18 22:23:55,745 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Receiving BP-1798474148-127.0.1.1-1563083204750:blk_1073741902_1078 src: /127.0.0.1:38110 dest: /127.0.0.1:50010
2019-07-18 22:23:55,864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace: src: /127.0.0.1:38110, dest: /127.0.0.1:50010, bytes: 1099915, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_2044258425_1, offset: 0, srvID: 76507605-3e89-4178-8acd-669991b4c7b4, blockid: BP-1798474148-127.0.1.1-1563083204750:blk_1073741902_1078, duration: 118536173
2019-07-18 22:23:55,864 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: PacketResponder: BP-1798474148-127.0.1.1-1563083204750:blk_1073741902_1078, type=LAST_IN_PIPELINE terminating
2019-07-18 22:32:39,044 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741874_1050 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741874 for deletion
2019-07-18 22:32:39,047 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741892_1068 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741892 for deletion
2019-07-18 22:32:39,048 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741893_1069 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741893 for deletion
2019-07-18 22:32:39,048 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741880_1056 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741880 for deletion
2019-07-18 22:32:39,048 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Scheduling blk_1073741886_1062 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741886 for deletion
2019-07-18 22:32:39,052 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741874_1050 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741874
2019-07-18 22:32:39,053 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741892_1068 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741892
2019-07-18 22:32:39,056 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741893_1069 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741893
2019-07-18 22:32:39,056 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741880_1056 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741880
2019-07-18 22:32:39,057 INFO org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService: Deleted BP-1798474148-127.0.1.1-1563083204750 blk_1073741886_1062 file /tmp/hadoop-nilesh/dfs/data/current/BP-1798474148-127.0.1.1-1563083204750/current/finalized/subdir0/subdir0/blk_1073741886
